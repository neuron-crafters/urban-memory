{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eddd8267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a4b189f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = os.getenv(\"LANGCHAIN_PROJECT\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = os.getenv(\"LANGCHAIN_TRACING_V2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62e20f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5364cc5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x00000235C9A16270> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x00000235C9A16CF0> root_client=<openai.OpenAI object at 0x00000235C9A14050> root_async_client=<openai.AsyncOpenAI object at 0x00000235C9A16A50> model_name='o3-mini' model_kwargs={} openai_api_key=SecretStr('**********') stream_usage=True\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(model=\"o3-mini\")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f2e530a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='\"Agentic AI\" is a term used to describe artificial intelligence systems that operate with a degree of autonomy and intentionality—essentially, AI that acts like an independent agent. Here are some key points to understand the concept:\\n\\n1. Autonomy and Goal-Directed Behavior: An agentic AI is designed to pursue certain goals or objectives on its own. Instead of merely following a fixed set of instructions passively, it takes the initiative to make decisions and take actions in pursuit of its goals, often in dynamic and open-ended environments.\\n\\n2. Decision-Making and Adaptability: Such systems are built to evaluate their surroundings, update their understanding as new data comes in, and modify their behavior accordingly. This capacity for adaptation means that agentic AIs can operate effectively in complex scenarios without needing constant external guidance.\\n\\n3. Contrast with Traditional AI: Traditional AI systems are often task-specific—performing programmed operations such as classification, prediction, or data processing in a controlled way. In contrast, agentic AI is geared toward broader autonomous function, sometimes resembling the way a human or an animal might pursue goals within an environment.\\n\\n4. Relevance to AI Safety and Alignment: The idea of agentic AI is central to discussions about advanced artificial intelligence and its potential risks. Because these systems can initiate actions independently, ensuring that their objectives align with human values (“AI alignment”) becomes especially crucial. Researchers and ethicists study agentic behaviors to prevent unintended consequences as AI systems become more capable.\\n\\n5. Examples and Research Directions: While many current AI systems do not fully possess agentic qualities (most are still highly specialized), research in machine learning, reinforcement learning, and multi-agent systems is incrementally pushing toward more autonomous, agent-like behavior. Autonomous vehicles, robotic systems, and even virtual assistants provide early examples of systems with agentic properties, though they are usually constrained by design parameters.\\n\\nIn summary, \"Agentic AI\" refers to systems that function as autonomous agents—with the ability to set, pursue, and adapt strategies to achieve their objectives. This concept marks a shift from narrow, task-specific tools to more flexible, goal-driven entities, underscoring both the possibilities for advanced AI applications and the challenges for ensuring safe and ethical deployment.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 908, 'prompt_tokens': 12, 'total_tokens': 920, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 448, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'o3-mini-2025-01-31', 'system_fingerprint': 'fp_6c43dcef8c', 'id': 'chatcmpl-CYqDGWCIHEWOMdQDWFCYZQtkeKPII', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--83a8cee0-c526-4835-91a5-663b133a8906-0' usage_metadata={'input_tokens': 12, 'output_tokens': 908, 'total_tokens': 920, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 448}}\n"
     ]
    }
   ],
   "source": [
    "result = llm.invoke(\"What is the Agentic AI\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fc486fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Agentic AI\" is a term used to describe artificial intelligence systems that operate with a degree of autonomy and intentionality—essentially, AI that acts like an independent agent. Here are some key points to understand the concept:\n",
      "\n",
      "1. Autonomy and Goal-Directed Behavior: An agentic AI is designed to pursue certain goals or objectives on its own. Instead of merely following a fixed set of instructions passively, it takes the initiative to make decisions and take actions in pursuit of its goals, often in dynamic and open-ended environments.\n",
      "\n",
      "2. Decision-Making and Adaptability: Such systems are built to evaluate their surroundings, update their understanding as new data comes in, and modify their behavior accordingly. This capacity for adaptation means that agentic AIs can operate effectively in complex scenarios without needing constant external guidance.\n",
      "\n",
      "3. Contrast with Traditional AI: Traditional AI systems are often task-specific—performing programmed operations such as classification, prediction, or data processing in a controlled way. In contrast, agentic AI is geared toward broader autonomous function, sometimes resembling the way a human or an animal might pursue goals within an environment.\n",
      "\n",
      "4. Relevance to AI Safety and Alignment: The idea of agentic AI is central to discussions about advanced artificial intelligence and its potential risks. Because these systems can initiate actions independently, ensuring that their objectives align with human values (“AI alignment”) becomes especially crucial. Researchers and ethicists study agentic behaviors to prevent unintended consequences as AI systems become more capable.\n",
      "\n",
      "5. Examples and Research Directions: While many current AI systems do not fully possess agentic qualities (most are still highly specialized), research in machine learning, reinforcement learning, and multi-agent systems is incrementally pushing toward more autonomous, agent-like behavior. Autonomous vehicles, robotic systems, and even virtual assistants provide early examples of systems with agentic properties, though they are usually constrained by design parameters.\n",
      "\n",
      "In summary, \"Agentic AI\" refers to systems that function as autonomous agents—with the ability to set, pursue, and adapt strategies to achieve their objectives. This concept marks a shift from narrow, task-specific tools to more flexible, goal-driven entities, underscoring both the possibilities for advanced AI applications and the challenges for ensuring safe and ethical deployment.\n"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "818946fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"<think>\\nOkay, the user said, 'Hi My name is Agam.' First, I need to respond politely and acknowledge his introduction. He might want to start a conversation or ask a question, so I should keep it open-ended to encourage him to continue. I'll make sure to use his name to make the interaction more personal. Let me check the response again to ensure it's friendly and inviting. Something like, 'Hi Agam! Nice to meet you. How can I assist you today?' Yeah, that sounds good. It's polite, acknowledges him by name, and leaves the door open for him to ask for help with anything.\\n</think>\\n\\nHi Agam! Nice to meet you. How can I assist you today?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 150, 'prompt_tokens': 14, 'total_tokens': 164, 'completion_time': 0.358463359, 'prompt_time': 0.000521086, 'queue_time': 0.442463943, 'total_time': 0.358984445}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_fbaaebd3ce', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--92cf67b3-d2dd-4d23-b207-4157b801534b-0', usage_metadata={'input_tokens': 14, 'output_tokens': 150, 'total_tokens': 164})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "model=ChatGroq(model=\"qwen/qwen3-32b\")\n",
    "\n",
    "model.invoke(\"Hi My name is Agam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b42d616",
   "metadata": {},
   "source": [
    "### Prompt Engineering\n",
    "\n",
    "#### Chat Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30b66f2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an expert AI Engineer. Provide me answer based on the question\"),\n",
    "        (\"user\", \"{input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0562814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x00000235CB7A4190>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x00000235CB7A4E10>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model=ChatGroq(model=\"llama-3.1-8b-instant\")\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10525101",
   "metadata": {},
   "source": [
    "### Chaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3f98783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x00000235CB7A4190>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x00000235CB7A4E10>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Chaining\n",
    "chain = prompt|model\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "866d208b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke({\"input\": \"Can you tell me something about Langsmith\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e2f1219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langsmith is a tool that utilizes AI to translate languages. It is still under development.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00378251",
   "metadata": {},
   "source": [
    "### Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a777950f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langsmith is a Python library that can be used for Natural Language Processing (NLP) tasks, particularly for machine translation. It is built on top of the Moses machine translation framework and supports various machine translation models, including phrase-based, syntax-based, and neural machine translation models.\n",
      "\n",
      "Langsmith provides several features, including:\n",
      "\n",
      "1. **Support for multiple machine translation models**: Langsmith supports various machine translation models, including Moses, NMT, and other custom models.\n",
      "2. **Language support**: Langsmith supports multiple languages, including English, Spanish, French, German, Chinese, and many others.\n",
      "3. **Pre-trained models**: Langsmith provides pre-trained models for various machine translation tasks, which can be used out-of-the-box.\n",
      "4. **Customizable**: Langsmith allows users to customize their machine translation models, including adding custom dictionaries, syntax rules, and other modifications.\n",
      "5. **Easy-to-use API**: Langsmith provides a simple and easy-to-use API for integrating machine translation capabilities into other applications.\n",
      "\n",
      "Some of the key features and functionalities of Langsmith include:\n",
      "\n",
      "- **Language detection**: Langsmith can detect the language of a given text.\n",
      "- **Text translation**: Langsmith can translate text from one language to another.\n",
      "- **Machine translation**: Langsmith supports various machine translation models, including phrase-based, syntax-based, and neural machine translation models.\n",
      "- **Sentence alignment**: Langsmith can align sentences from two languages, which is useful for machine translation and other NLP tasks.\n",
      "\n",
      "Overall, Langsmith is a powerful tool for machine translation and other NLP tasks, and it provides a flexible and customizable API for integrating machine translation capabilities into other applications.\n",
      "\n",
      "### Example Usage:\n",
      "Here is an example of how to use Langsmith for text translation:\n",
      "```python\n",
      "from langsmith import Langsmith\n",
      "\n",
      "# Initialize the Langsmith object\n",
      "langsmith = Langsmith()\n",
      "\n",
      "# Define the source and target languages\n",
      "src_lang = \"en\"\n",
      "tgt_lang = \"fr\"\n",
      "\n",
      "# Define the text to translate\n",
      "text = \"Hello, how are you?\"\n",
      "\n",
      "# Perform the translation\n",
      "translated_text = langsmith.translate(text, src_lang, tgt_lang)\n",
      "\n",
      "# Print the translated text\n",
      "print(translated_text)\n",
      "```\n",
      "This code initializes a Langsmith object, defines the source and target languages, and performs the translation of the given text. The translated text is then printed to the console.\n"
     ]
    }
   ],
   "source": [
    "### OutputParser\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser=StrOutputParser()\n",
    "\n",
    "chain=prompt|model|output_parser\n",
    "\n",
    "response = chain.invoke({\"input\": \"Can you tell me something about Langsmith\"})\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa170a0d",
   "metadata": {},
   "source": [
    "### Json Output parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82274307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Return a JSON object.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### JsonOutputParser\n",
    "\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "output_parser=JsonOutputParser()\n",
    "\n",
    "output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7861f463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['query'], input_types={}, partial_variables={'format_instruction': 'Return a JSON object.'}, template='Answer the user query \\n {format_instruction} \\n {query} \\n')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query \\n {format_instruction} \\n {query} \\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instruction\":output_parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db34a74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Langsmith', 'definition': 'Langsmith is a platform that provides AI-powered content creation and translation tools.', 'features': ['Content creation: Langsmith offers a range of tools for creating high-quality content, including articles, product descriptions, and social media posts.', 'Translation: The platform provides accurate and culturally relevant translations for businesses that operate globally.', \"Content optimization: Langsmith's AI algorithms analyze and optimize content for better engagement, conversion, and SEO.\", 'Integration: Langsmith integrates with various platforms and tools, making it easy to incorporate its features into existing workflows.'], 'benefits': ['Increased efficiency: Langsmith automates content creation and translation, saving businesses time and resources.', \"Improved quality: The platform's AI-powered tools ensure high-quality content that resonates with target audiences.\", \"Enhanced global reach: Langsmith's translation capabilities help businesses connect with customers worldwide, expanding their global footprint.\"]}\n"
     ]
    }
   ],
   "source": [
    "chain = prompt|model|output_parser\n",
    "response = chain.invoke({\"query\": \"Can you tell me something about Langsmith\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad73c84",
   "metadata": {},
   "source": [
    "#### Example with ChatPrompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e7e34c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide the response is in Json. Provide me answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an expert AI Engineer. Provide the response is in Json. Provide me answer based on the question\"),\n",
    "        (\"user\", \"{input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df101fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"LangSmith\",\n",
      "  \"description\": \"LangSmith is an AI-powered language learning platform\",\n",
      "  \"key_features\": [\n",
      "    \"Personalized learning plans\",\n",
      "    \"Interactive lessons\",\n",
      "    \"Real-time feedback\",\n",
      "    \"Customizable learning pace\",\n",
      "    \"Access to native speaker community\"\n",
      "  ],\n",
      "  \"target_audience\": [\n",
      "    \"Language learners\",\n",
      "    \"Students\",\n",
      "    \"Professionals\",\n",
      "    \"Travelers\"\n",
      "  ],\n",
      "  \"available_languages\": [\n",
      "    \"English\",\n",
      "    \"Spanish\",\n",
      "    \"French\",\n",
      "    \"German\",\n",
      "    \"Chinese\",\n",
      "    \"Japanese\",\n",
      "    \"Korean\",\n",
      "    \"Arabic\",\n",
      "    \"Russian\",\n",
      "    \"Portuguese\"\n",
      "  ],\n",
      "  \"platforms\": [\n",
      "    \"Web\",\n",
      "    \"Mobile App (iOS and Android)\"\n",
      "  ],\n",
      "  \" pricing\": [\n",
      "    \"Free trial\",\n",
      "    \"Subscription plans (monthly and yearly)\"\n",
      "  ],\n",
      "  \"integrations\": [\n",
      "    \"Google Calendar\",\n",
      "    \"Trello\",\n",
      "    \"Slack\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "chain = prompt|model|output_parser\n",
    "response = chain.invoke({\"input\":\"Can you tell me about LangSmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2153357c",
   "metadata": {},
   "source": [
    "### XML Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b8e17426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide the response is in <response><answer>Your answer here</answer></response>. Provide me answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import XMLOutputParser\n",
    "\n",
    "output_parser=XMLOutputParser()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an expert AI Engineer. Provide the response is in <response><answer>Your answer here</answer></response>. Provide me answer based on the question\"),\n",
    "        (\"user\", \"{input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "34c02a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='<response>\\n<answer>LangSmith is an AI-powered language translation tool that was launched in 2023. It was developed by Meta, a technology company known for its social media platforms like Facebook and Instagram. LangSmith is designed to provide high-quality, human-like translations across various languages and domains. The tool utilizes advanced machine learning algorithms to learn from large datasets and improve its translation accuracy over time. LangSmith aims to bridge the language gap and facilitate communication across languages, cultures, and communities.</answer>\\n</response>' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 106, 'prompt_tokens': 74, 'total_tokens': 180, 'completion_time': 0.194217631, 'prompt_time': 0.003944746, 'queue_time': 0.055473174, 'total_time': 0.198162377}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ab04adca7d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--b8ff6a42-99e4-44d2-b7a3-04230bd4be8e-0' usage_metadata={'input_tokens': 74, 'output_tokens': 106, 'total_tokens': 180}\n"
     ]
    }
   ],
   "source": [
    "chain = prompt|model\n",
    "response = chain.invoke({\"input\":\"Can you tell me about LangSmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0b671f94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['query'], input_types={}, partial_variables={'format_instruction': 'The output should be formatted as a XML file.\\n1. Output should conform to the tags below.\\n2. If tags are not given, make them on your own.\\n3. Remember to always open and close all the tags.\\n\\nAs an example, for the tags [\"foo\", \"bar\", \"baz\"]:\\n1. String \"<foo>\\n   <bar>\\n      <baz></baz>\\n   </bar>\\n</foo>\" is a well-formatted instance of the schema.\\n2. String \"<foo>\\n   <bar>\\n   </foo>\" is a badly-formatted instance.\\n3. String \"<foo>\\n   <tag>\\n   </tag>\\n</foo>\" is a badly-formatted instance.\\n\\nHere are the output tags:\\n```\\nNone\\n```'}, template='Answer the user query \\n {format_instruction}\\n {query}\\n ')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### OutputParser\n",
    "from langchain_core.output_parsers import XMLOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "output_parser=XMLOutputParser()\n",
    "\n",
    "prompt=PromptTemplate(\n",
    "    template=\"Answer the user query \\n {format_instruction}\\n {query}\\n \",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instruction\":output_parser.get_format_instructions()},\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9245b202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
      "<root>\n",
      "    <information>\n",
      "        <title>Langsmith</title>\n",
      "        <description>Langsmith is not a well-known term. However, it could be inferred as a combination of \"Language\" and \"Smith\". A smith is a person who works with metal to create tools or other objects. Therefore, a Langsmith could be a person who works with language to create or shape words, syntax, or semantics.</description>\n",
      "    </information>\n",
      "</root>\n"
     ]
    }
   ],
   "source": [
    "chain=prompt|model\n",
    "response=chain.invoke({\"query\":\"Can you tell me about Langsmith?\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "520c1444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='<response><answer>LangChain is an open-source Python framework for building large language models (LLMs) applications. It aims to simplify the development process by providing a set of tools and libraries that make it easier to integrate and manage LLMs in various use cases, such as chatbots, question answering, and text generation. LangChain provides a range of features, including model management, data loading, and workflow orchestration, making it a powerful tool for developers and researchers working with LLMs.</answer></response>' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 107, 'prompt_tokens': 64, 'total_tokens': 171, 'completion_time': 0.162083343, 'prompt_time': 0.003424699, 'queue_time': 0.05009701, 'total_time': 0.165508042}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_7b3cfae3af', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--47c3bbb2-aaaa-4bd9-81ce-447c61fe4192-0' usage_metadata={'input_tokens': 64, 'output_tokens': 107, 'total_tokens': 171}\n"
     ]
    }
   ],
   "source": [
    "##output parser\n",
    "from langchain_core.output_parsers import XMLOutputParser\n",
    "#from langchain.output_parsers.xml import XMLOutputParser\n",
    "\n",
    "# XML Output Parser\n",
    "output_parser = XMLOutputParser()\n",
    "\n",
    "# Prompt that instructs the model to return XML\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Respond in this XML format: <response><answer>Your answer here</answer></response>\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Build the chain\n",
    "chain = prompt | model\n",
    "\n",
    "# Run the chain\n",
    "#response = chain.invoke({\"input\": \"What is LangChain?\"})\n",
    "\n",
    "raw_output =chain.invoke({\"input\": \"What is LangChain?\"})\n",
    "\n",
    "# Print result\n",
    "print(raw_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1527ce1c",
   "metadata": {},
   "source": [
    "### Object parsers With Pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "24f34e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'setup': 'Why did the math book look sad?',\n",
       " 'punchline': 'Because it had too many problems.'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## With Pydantic\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "model = ChatOpenAI(temperature=0.7)\n",
    "\n",
    "\n",
    "# Define your desired data structure.\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "\n",
    "# And a query intented to prompt a language model to populate the data structure.\n",
    "joke_query = \"Tell me a joke for kids.\"\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = JsonOutputParser(pydantic_object=Joke)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\"query\": joke_query})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d627d83c",
   "metadata": {},
   "source": [
    "### Object parsers without pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "59261860",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'response': \"Sure! Here's a joke for kids: Why was the math book sad? Because it had too many problems.\"}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Without Pydantic\n",
    "\n",
    "model = ChatOpenAI(temperature=0.7)\n",
    "\n",
    "# And a query intented to prompt a language model to populate the data structure.\n",
    "joke_query = \"Tell me a joke for kids.\"\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = JsonOutputParser()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\"query\": joke_query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60300f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<movie>Big</movie>\n",
      "<movie>Forrest Gump</movie>\n",
      "<movie>Saving Private Ryan</movie>\n",
      "<movie>Cast Away</movie>\n",
      "<movie>The Da Vinci Code</movie>\n",
      "<movie>Toy Story</movie>\n",
      "<movie>Philadelphia</movie>\n",
      "<movie>Sully</movie>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_core.output_parsers import XMLOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "actor_query = \"Generate the shortened filmography for Tom Hanks.\"\n",
    "\n",
    "output = model.invoke(\n",
    "    f\"\"\"{actor_query}\n",
    "Please enclose the movies in <movie></movie> tags\"\"\"\n",
    ")\n",
    "\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511e4ae7",
   "metadata": {},
   "source": [
    "### Yaml parser \n",
    "\n",
    "- This parser is Removed from version 1.0.3\n",
    "- To keep it working we need to install langchain-classic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "331bc863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Joke(setup=\"Why couldn't the bicycle stand up by itself?\", punchline='Because it was two tired!')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_classic.output_parsers import YamlOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "# Define your desired data structure.\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "\n",
    "model = ChatOpenAI(temperature=0.5)\n",
    "\n",
    "# And a query intented to prompt a language model to populate the data structure.\n",
    "joke_query = \"Tell me a joke.\"\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = YamlOutputParser(pydantic_object=Joke)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\"query\": joke_query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589daf80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
